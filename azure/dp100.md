Deep Learning

Activation functions:
1. ReLU - Rectilinear Unit
- ensure negatives are turned to 0
2. SoftMax
- outputs vector of probabilities that sum to 1

Sample NN:
```
import tensorflow
from tensorflow import keras
from tensorflow.keras import models
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

hl = 10 # Number of hidden layer nodes

model = Sequential()
model.add(Dense(hl, input_dim=len(features), activation='relu'))
model.add(Dense(hl, input_dim=hl, activation='relu'))
model.add(Dense(len(penguin_classes), input_dim=hl, activation='softmax'))

print(model.summary())
```

```
#hyper-parameters for optimizer
learning_rate = 0.001
opt = optimizers.Adam(lr=learning_rate)

model.compile(loss='categorical_crossentropy',
              optimizer=opt,
              metrics=['accuracy'])

# Train the model over 50 epochs using 10-observation batches and using the test holdout dataset for validation
num_epochs = 50
history = model.fit(x_train, y_train, epochs=num_epochs, batch_size=10, validation_data=(x_test, y_test))
```

```
# Save the trained model
modelFileName = 'models/penguin-classifier.h5'
model.save(modelFileName)
del model  # deletes the existing model variable
print('model saved as', modelFileName)

model = models.load_model(modelFileName)
class_probabilities = model.predict(x_new)
```

### Ways to run
```
from azureml.core import Experiment
experiment = Experiment(workspace=ws, name='name')
run = experiment.start_logging()
run.log('name', value)
...
run.complete()
```

```
%%writefile filename.py

from azureml.core import Run

run = Run.get_context()
run.log(...)
...
run.complete()
```

```
from azureml.core import Experiment, ScriptRunConfig

scriptrunconfig = ScriptRunConfig(
  source_directory, 
  script=None, 
  arguments=None, 
  run_config=None, 
  _telemetry_values=None, 
  compute_target=None, 
  environment=None, 
  distributed_job_config=None, 
  resume_from=None, 
  max_run_duration_seconds=2592000, 
  command=None, 
  docker_runtime_config=None
)
experiment = Experiment(workspace, name)
run = experiment.submit(config=scriptrunconfig)
run.wait_for_completion()
```

#### Using mlflow

```
from azureml.core import Experiment
import mlflow

mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())

experiment = Experiment(workspace, name)
mlflow.set_experiment(experiment.name)

with mlflow.start_run():
    ...
    mlflow.log_metric('observations', row_count)
```

```
run = list(experiment.get_runs())[0]
experiment_url = experiment.get_portal_url()
```

#### mlflow in a script
```
%%writefile filename.py

import mlflow
with mlflow.start_run():
  ...
  mlflow.log_metric(name, value)
```

```
from azureml.core import Experiment, Environment, ScriptRunConfig
from azureml.core.conda_dependencies import CondaDependencies

env = Environment(name)

packages = CondaDependencies.create(conda_packages=['pandas', 'pip],
  pip_packages=['mlflow', 'azureml-mlflow']
)

env.python.conda_dependencies = packages

config = ScriptRunConfig(
  source_directory,
  script=script.py,
  environment=env
)
experiment = Experiment(workspace, name)
run = experiment.submit(config=config)
run.wait_for_completion()
```

#### To run sklearn
```
from azureml.core import Experiment, Environment, ScriptRunConfig
from azureml.core.conda_dependencies import CondaDependencies

env = Environment(name)

packages = CondaDependencies.create(pip_pacakges=['scikit-learn', 'azureml-defaults'])
env.python.conda_dependencies = packages 

config = ScriptRunConfig(
  source_directory,
  script,
  environment
)

exp = Experiment(workspace, name)
run = exp.submit(config)
run.wait_for_completion()
```

### interactive run widget
```
from azureml.widgets import RunDetails

RunDetails(run).show()
```

### register model

```
from azureml.core import Model

run.register_model(model_path, model_name, tags={}, properties={})
models = Model.list(workspace)
```

### script with argument
```
%%writefile script.py

import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--argument', type=float, dest=variable, default=value)
args = parser.parse_args()
variable = args.variable
```

```
from azureml.core import ScriptRunConfig

config = ScriptRunConfig(
  source_directory,
  script,
  arguments=['--arg1', arg1_val, '--arg2', arg2_val],
  environment
)
```

#### datastores
```
ds = ws.get_default_datastore()
ds.upload_files(files=[], target_path='./',
  overwrite=True,
  show_progress=True
)
```

#### datasets
```
from azureml.core import Dataset

tab = Dataset.Tabular.from_delimited_files(path=(ds, 'path/*.csv'))
tab.to_pandas_dataframe()

file = Dataset.File.from_files(path=(ds, 'path/*.csv))
file.to_path()
```

```
tab = tab.register(workspace,
  name,
  description,
  tags,
  create_new_version=True
)

file = file.register(workspace,
  name,
  description,
  tags,
  create_new_version=True
)
```

```
tab = Dataset.get_by_name(ws, name, version)
tab = ws.datasets.get(name)
```

### dataset as input arguments
```
ds = ws.datasets.get(name)
script_config = ScriptRunConfig(source_directory,
                              script,
                              arguments = ['--input-data', ds.as_named_input('training_data')],
                              environment) 
```

```
ds = run.input_datasets['training_data'].to_pandas_dataframe()
```

#### models
```
import joblib

joblib.dump(value=model, filename='path/model.pkl')
```

### docker environment
```
from azureml.core import Environment
from azureml.core.conda_dependencies import CondaDependencies

env = Environment(name)
env.python.user_managed_dependencies = False ### Azure-managed dependencies
env.docker.enabled = True ### use a docker container

packages = CondaDependencies.create(pip_packages=[], conda_packages=[])

env.python.conda_dependencies = packages
```

### register the environment
```
env.register(workspace)
```

### read environment details
```
env.python.conda_dependencies.serialize_to_string()
```

### compute target
```
from azureml.core import ComputeTarget, AmlCompute
cluster = ComputeTarget(ws, name) # get compute target
cluster_config = AmlCompute.provisioning_configuration(
  vm_size='STANDARD_DS11_V2',
  max_nodes=2
)
cluster = ComputeTarget.create(ws, name, compute_config=cluster_config)
cluster.wait_for_completion()
```

### run on remote compute
```
script_config = ScriptRunConfig(source_directory=experiment_folder,
                                script='diabetes_training.py',
                                arguments = ['--input-data', diabetes_ds.as_named_input('training_data')],
                                environment=registered_env,
                                compute_target=cluster_name) 
```

#### create a pipeline
```
from azureml.core.runconfig import RunConfiguration

runconfig = RunConfiguration()
runconfig.target = compute_target
runconfig.environment = registered_environment
```

```
from azureml.pipeline.core import PipelineData
from azureml.pipeline.steps import PythonScriptStep

pipeline_date = PipelineDate(name, datastore)
step = PythonScriptStep(name,
  source_directory,
  script_name,
  arguments,
  outputs=[pipeline_data]
  compute_target,
  runconfig,
  allow_reuse=True
)
```

```
from azureml.pipeline.core import Pipeline

steps = [step, step]
pipeline = Pipeline(workspace, steps)
run = Experiment.submit(pipeline, regenerate_outputs=True)
```

### publish pipeline
```
published_pipeline = run.publish_pipeline(name, description, version)
published_pipeline.endpoint
```

#### auth
```
from azureml.core.authentication import InteractiveLoginAuthentication

interactive_auth = InteractiveLoginAuthentication()
auth_header = interactive_auth.get_authentication_header()
```

```
import requests
response = requests.post(rest_endpoint, 
                         headers=auth_header, 
                         json={"ExperimentName": experiment_name})
```

### track async pipeline run
```
from azureml.pipeline.core.run import PipelineRun

run = PipelineRun(experiment, run_id)
run.wait_for_completion(show_output=True)
```

### schedule
```
from azureml.pipeline.core import ScheduleRecurrence, Schedule

recurrence = ScheduleRecurrence(frequency, interval, etc)
sched = Schedule.create(ws,
  name,
  pipeline_id,
  experiment_name,
  recurrence
)
```